{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62988d77-d8ab-418a-8593-045676521095",
   "metadata": {
    "id": "62988d77-d8ab-418a-8593-045676521095"
   },
   "source": [
    "# Named Entity Recognition with DistilBERT\n",
    "\n",
    "\n",
    "This notebook fine-tunes **DistilBERT** (`distilbert-base-uncased`) using Hugging Face Transformers to perform **Named Entity Recognition (NER)**.\n",
    "The goal is to identify entities such as **persons, organizations, locations, and dates** in text.\n",
    "\n",
    "\n",
    "**Key steps in this notebook:**\n",
    "1. Load and preprocess dataset.\n",
    "2. Tokenize text using DistilBERT tokenizer.\n",
    "3. Fine-tune DistilBERT for token classification.\n",
    "4. Evaluate model qualitatively.\n",
    "5. Visualize sample predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03aec8-2efa-49d6-9eb9-f3e027fd1841",
   "metadata": {
    "id": "3b03aec8-2efa-49d6-9eb9-f3e027fd1841",
    "outputId": "5ab384d1-0d71-4e84-e664-928f17acddf5"
   },
   "source": [
    "##### Install pytorch_lightning (if not already installed)\n",
    "- Run: !pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a69b379-02e6-416e-a4b1-167bbd216a7c",
   "metadata": {
    "id": "8a69b379-02e6-416e-a4b1-167bbd216a7c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc78ca7-ac41-4f10-a5c8-eccd1a367f01",
   "metadata": {
    "id": "8dc78ca7-ac41-4f10-a5c8-eccd1a367f01"
   },
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, label2id, tokenizer, max_len=128):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.label2id = label2id\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.sentences[idx], self.labels[idx]\n",
    "        enc = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        word_ids = enc.word_ids(batch_index=0)\n",
    "        labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                labels.append(-100)\n",
    "            else:\n",
    "                labels.append(self.label2id[tags[word_id]])\n",
    "        enc = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        enc['labels'] = torch.tensor(labels)\n",
    "        return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa7f98-6b54-4176-a3cc-0669d044bcfa",
   "metadata": {
    "id": "37fa7f98-6b54-4176-a3cc-0669d044bcfa"
   },
   "source": [
    "## Model: DistilBERT for Token Classification\n",
    "\n",
    "\n",
    "We use Hugging Face Transformers to load `distilbert-base-uncased` with a classification head for NER.\n",
    "\n",
    "\n",
    "- **Base Model**: DistilBERT encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b38541e-8a65-4b9e-937a-81110e918ba3",
   "metadata": {
    "id": "8b38541e-8a65-4b9e-937a-81110e918ba3"
   },
   "outputs": [],
   "source": [
    "class LitTokenClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name, id2label):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(id2label),\n",
    "            id2label=id2label,\n",
    "            label2id={v:k for k,v in id2label.items()}\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        self.log(\"train_loss\", outputs.loss)\n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        self.log(\"val_loss\", outputs.loss, prog_bar=True)\n",
    "        return outputs.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53fe4cad-d706-4185-a894-181a8ccc6091",
   "metadata": {
    "id": "53fe4cad-d706-4185-a894-181a8ccc6091"
   },
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, words):\n",
    "    enc = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.model(**enc)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "    pred_tags = []\n",
    "    for idx, word_id in enumerate(enc.word_ids()):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        label_id = preds[0, idx].item()\n",
    "        pred_tags.append(model.model.config.id2label[label_id])\n",
    "    return pred_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7481d32-cfdb-41ec-a802-d64e86ca6b9b",
   "metadata": {
    "id": "d7481d32-cfdb-41ec-a802-d64e86ca6b9b"
   },
   "source": [
    "## Dataset & Preprocessing\n",
    "\n",
    "- Dataset from: DeepLearning.AI\n",
    "- Each sample contains a sequence of tokens with corresponding NER tags.\n",
    "\n",
    "A few tags you might expect to see are:\n",
    "* `geo`: geographical entity\n",
    "* `org`: organization\n",
    "* `per`: person\n",
    "* `gpe`: geopolitical entity\n",
    "* `tim`: time indicator\n",
    "* `art`: artifact\n",
    "* `eve`: event\n",
    "* `nat`: natural phenomenon\n",
    "* `O`: filler word\n",
    "\n",
    "\n",
    "**Preprocessing steps:**\n",
    "- Load dataset using Hugging Face `datasets` library.\n",
    "- Tokenize sentences with DistilBERT tokenizer.\n",
    "- Align tokenized inputs with entity labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e7da23-93a5-4c08-8167-e7ae15401a8c",
   "metadata": {
    "id": "73e7da23-93a5-4c08-8167-e7ae15401a8c"
   },
   "outputs": [],
   "source": [
    "def load_dataset(sent_file, label_file):\n",
    "    sentences = [line.strip().split() for line in open(sent_file, \"r\")]\n",
    "    labels = [line.strip().split() for line in open(label_file, \"r\")]\n",
    "    return sentences, labels\n",
    "\n",
    "# Load train/val/test\n",
    "train_sentences, train_labels = load_dataset(\"data/train_sentences.txt\", \"data/train_labels.txt\")\n",
    "val_sentences, val_labels = load_dataset(\"data/val_sentences.txt\", \"data/val_labels.txt\")\n",
    "test_sentences, test_labels = load_dataset(\"data/test_sentences.txt\", \"data/test_labels.txt\")\n",
    "\n",
    "tags = ['B-art','B-eve','B-geo','B-gpe','B-nat','B-org','B-per','B-tim',\n",
    "        'I-art','I-eve','I-geo','I-gpe','I-nat','I-org','I-per','I-tim','O']\n",
    "\n",
    "label2id = {tag: i for i, tag in enumerate(tags)}\n",
    "id2label = {i: tag for tag, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbd049b-a3bc-453b-98d5-559810f93107",
   "metadata": {
    "id": "cfbd049b-a3bc-453b-98d5-559810f93107"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c5781bc-7588-47ee-be7e-8737880f7681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663,
     "referenced_widgets": [
      "7b08b67a76e84f5e95d4cfb68e5f79da",
      "aeaa2871a1774d2d98d809657e00765c",
      "1a7d53a7f322444d8283f07fef91919c",
      "e2b05104a4e94fe9994614a360be3ea7",
      "8d821a5515db43aea430941898f5df64",
      "39cabfdf2a454b53be3e54717e72fb22",
      "b4a394e202064449958dda685f33304f",
      "7ad2a4fb506145a7980a70cc37102543",
      "f8be2e9f842745eabfbb90e4150c0aaf",
      "eedea6e15ea9438ca5544a64f7a2db00",
      "2a6c59cb611e495bbb0c53d3d884adf5",
      "a497037beb9e472a865ad95c15be0e69",
      "2c275dab1e9347b4bf01aa1b0eae17d0",
      "df00ceeef32a4d5bb6916cc395286768",
      "c049ab43fe5144b1bd8b6d8207916f75",
      "bb0b8cef4cf8403d8e7a7821d15992f7",
      "7ecde80108484726b1acf477cecd4297",
      "3320dfdd7ced4425ae5a3cfb3163edc6",
      "9a2642996f684f0a83a922c59924a257",
      "d1a2fa38acfe4ce08033dd749d7dd89c",
      "ba552dac1a354aa59cf12205ee8f7c55",
      "11dd1e3ba9994cf6b2319939c05dfc6c",
      "33a86c22f672405184ae21af14bd9fd1",
      "108d3555c1f14e3a88c6b1ac31217e0d",
      "346fbce681074795892a935c8cf8ca7b",
      "466df8c2fe0b461190afdf4d72988dc7",
      "a3a70784ea054751bc49bf2699e9dbc2",
      "bdbf818d5d084d6eb5b4d7fd352fcb50",
      "ca0c58e6bcf94adebbcad86b428f0d32",
      "2a65f773f5ca45b6b0ddc4ca623db697",
      "28e6bd70e08f470cb45efcba1943717e",
      "953dd5c2e7234d659689ad1d709b2b6f",
      "607b25c9054346c99141ca61c41080ae",
      "d46e4a6cca3a4f5abd748a573a1bf9b8",
      "894405c97079458ea936241bd51aaec6",
      "736fbd55472a4cf3b56e4cd18f185ce6",
      "0d8a18b7c67848adb2df3f9f3f4a1afd",
      "91a780770c7d46ba8578b4a56a25b895",
      "1574871c1771437e8538d360c65d4ca6",
      "7ea5d1242c1b4d4996e54ce6d51a7c18",
      "de36e531a7e5490ba1f8fc1ea29958d3",
      "daef2b1718e9415f97da64c1eed7e243",
      "fc52ef2d2d3a40ce8b65717b2ffd6d83",
      "97e61cdabc85410695b8d6870476f3e3",
      "67d537d969b041b6bf1855c9ff2caa4f",
      "5ddd319a18154b1abdb6cec9d2393d35",
      "61644ac38dca48669356707787bec619",
      "3842d932b2d2432da9d760de434b3b3c",
      "5e530699dd7a4efda075c94f452988c1",
      "ba5c99f46ac8477e997167300566e790",
      "dfb33980967b47a8b8da98babbb1c492",
      "9934e116a9af46059008c585695b3a25",
      "a422701e2baf4d518362a7f54a6ff517",
      "dee5f12405e74c5ea7e8e43e4eb3bfac",
      "d7c929d66da6406696ae87e554e80283",
      "e9bccd522773431ba03b8e8c72a7fe98",
      "14cb0352d4784783b1311f4b47ec0546",
      "7954058134d44d4d90938cb04b6348b9",
      "c06c6c6535d141478f54dd08418ef473",
      "d4d678c6599b41b99e87e8f26abf31ba",
      "8c3d552a3036441c959137e189a43558",
      "61c1a5d924b348e48aee889db10b368a",
      "89ce76c67caf47bcb315714544888abf",
      "74e622edc11749c8a6b144cc5b3cc7c9",
      "c74e5e961830490f9ac554d38b2e16b6",
      "4f227255b490421f85e51b651433eeb2",
      "d2a3f977ef014519b533849c31bcd516",
      "b34ee8c9f0e8413483dd8cc7ead4314a",
      "fbd373ec9ac54a4cabe220eda9366cee",
      "2619c0c6acc24d43a310bf997c8e8518",
      "612ee0abea654b08a4fdf23fad375542",
      "eb1b7f7cbf2c43c292d88a696d1cd992",
      "5a1cac75438145b6acc6526f534c6445",
      "d74ca65ab58144ada0f0a43a886b7a24",
      "5d83d6a023244d58b2c161d61a24359d",
      "f8a008b193fe4a79aaee431e98448eaa",
      "62904e5dc23d40648f8617fbb517cc57",
      "b514fbab60444da6956d2fdaf955ebcb",
      "f6b18ab9d7f043d2b29ef1e922e9399c",
      "eded18b33fe744b29dd8129e4d22c268",
      "846bb9f9affe485ebe1425b2176a97c6",
      "a3b6f6d567f445cc9fb2268311dec9d2",
      "6b5db87e33c84687912f1a0b676d232a",
      "cba1077f22bc448d90af9be7591cf003",
      "c2aebc1140e644fc8901821dcd3d23bb",
      "bed807084cbc49ab81abeabf1248e482",
      "eeb5e4bf759540fdb2db442804ee8215",
      "019e6161248c4e979d18a67802f371a4",
      "7bb273ee415e4cb7a5d8f976f87fba81",
      "50939730e91d401dbad05b3dbbf28d31",
      "566e82fa88354910b8dc0bb50c4c4467",
      "400fbe95f554470f8bb86452c107e769",
      "18c6fd384d3f440d8f76422462d08355",
      "2bddff540aee450787a1ad3512e62eb8",
      "12b22a8bf4904aa68bc1e8cdbee8773f",
      "65f29b3aaffa429bb3b90bf70d672f36",
      "ce24562f010e40e6a9ecf4805297db7f",
      "5028a58d4ac54250bc21448f0be9cd52",
      "d3809717d0fb40c691a1c6a6ebc045a9",
      "b4fbd31e6765440fb8666bfca2c8cc64",
      "47e4eaaff0ef48268457a34f94c07766",
      "feaf69c4d13046ee8254b8d821db2860",
      "e34a937d5450437f87290ffbc9bdc378",
      "8ff413a8927848918993709c1d083b60",
      "0008582109534521a399aadd4a3c743a",
      "fb0c8473dc05436c9abd34d19c10f4b1",
      "b5687f902ae341d5bfea52b3661c2f34",
      "7543758eba5745049d32085f05f5a347",
      "bafac67194184b9a848a8ba44317d222",
      "252bc9cc8ddb4e59a8df7264ff325223"
     ]
    },
    "id": "8c5781bc-7588-47ee-be7e-8737880f7681",
    "outputId": "ddd00f75-9f76-4972-be5a-b82973acfd74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b08b67a76e84f5e95d4cfb68e5f79da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a497037beb9e472a865ad95c15be0e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a86c22f672405184ae21af14bd9fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46e4a6cca3a4f5abd748a573a1bf9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d537d969b041b6bf1855c9ff2caa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type                             | Params | Mode\n",
      "------------------------------------------------------------------\n",
      "0 | model | DistilBertForTokenClassification | 66.4 M | eval\n",
      "------------------------------------------------------------------\n",
      "66.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "66.4 M    Total params\n",
      "265.504   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "95        Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bccd522773431ba03b8e8c72a7fe98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a3f977ef014519b533849c31bcd516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b514fbab60444da6956d2fdaf955ebcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb273ee415e4cb7a5d8f976f87fba81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fbd31e6765440fb8666bfca2c8cc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_dataset = NERDataset(train_sentences, train_labels, label2id, tokenizer)\n",
    "val_dataset   = NERDataset(val_sentences, val_labels, label2id, tokenizer)\n",
    "test_dataset  = NERDataset(test_sentences, test_labels, label2id, tokenizer)\n",
    "\n",
    "collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=16, collate_fn=collator)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=16, collate_fn=collator)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = 'gpu'\n",
    "    devices = 1\n",
    "else:\n",
    "    acclerator = 'cpu'\n",
    "    devices = 'auto'\n",
    "\n",
    "lit_model = LitTokenClassifier(\"distilbert-base-uncased\", id2label)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=3, accelerator=accelerator)\n",
    "trainer.fit(lit_model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee15a0-e7d2-450f-b2eb-0b59bc3a5be5",
   "metadata": {
    "id": "83ee15a0-e7d2-450f-b2eb-0b59bc3a5be5"
   },
   "source": [
    "#### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2903cfcc-121b-4282-8575-3c1db7198d1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2903cfcc-121b-4282-8575-3c1db7198d1d",
    "outputId": "0129580d-e152-4b33-d521-a371d53d1c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words: ['During', 'a', 'visit', 'to', 'Hungary', 'Tuesday', ',', 'Mr.', 'Putin', 'said', 'it', 'is', 'quite', 'possible', 'to', 'reach', 'agreement', 'on', 'Moscow', \"'s\", 'proposal', 'to', 'enrich', 'uranium', 'on', 'Russian', 'soil', 'for', 'Iran', \"'s\", 'nuclear', 'energy', 'needs', '.']\n",
      "\n",
      "Predicted: ['O', 'O', 'O', 'O', 'B-geo', 'B-tim', 'O', 'B-per', 'B-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "True: ['O', 'O', 'O', 'O', 'B-geo', 'B-tim', 'O', 'B-per', 'I-per', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "test_words = test_sentences[48]\n",
    "print(\"\\nWords:\", test_words)\n",
    "print(\"\\nPredicted:\", predict(lit_model, tokenizer, test_words))\n",
    "print(\"\\nTrue:\", test_labels[48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "MqiHeDwrq2MP",
   "metadata": {
    "id": "MqiHeDwrq2MP"
   },
   "outputs": [],
   "source": [
    "def word_to_label(sentence_idx, lit_model=lit_model, tokenizer=tokenizer):\n",
    "    words = test_sentences[sentence_idx]\n",
    "    labels = test_labels[sentence_idx]\n",
    "    preds = predict(lit_model, tokenizer, words)\n",
    "\n",
    "    for word, label, pred in zip(words, labels, preds):\n",
    "        print(f\"{word:15} {label:7} → {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "HV8oTig0sl35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HV8oTig0sl35",
    "outputId": "99e65b3b-122f-4092-f797-e05bc0ed3df5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During          O       → O\n",
      "a               O       → O\n",
      "visit           O       → O\n",
      "to              O       → O\n",
      "Hungary         B-geo   → B-geo\n",
      "Tuesday         B-tim   → B-tim\n",
      ",               O       → O\n",
      "Mr.             B-per   → B-per\n",
      "Putin           I-per   → B-per\n",
      "said            O       → I-per\n",
      "it              O       → O\n",
      "is              O       → O\n",
      "quite           O       → O\n",
      "possible        O       → O\n",
      "to              O       → O\n",
      "reach           O       → O\n",
      "agreement       O       → O\n",
      "on              O       → O\n",
      "Moscow          B-geo   → O\n",
      "'s              O       → B-geo\n",
      "proposal        O       → O\n",
      "to              O       → O\n",
      "enrich          O       → O\n",
      "uranium         O       → O\n",
      "on              O       → O\n",
      "Russian         B-gpe   → O\n",
      "soil            O       → O\n",
      "for             O       → O\n",
      "Iran            B-geo   → B-gpe\n",
      "'s              O       → O\n",
      "nuclear         O       → O\n",
      "energy          O       → B-geo\n",
      "needs           O       → O\n",
      ".               O       → O\n"
     ]
    }
   ],
   "source": [
    "word_to_label(48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b688fa-759b-4294-bf05-0b3f296a362b",
   "metadata": {
    "id": "c6b688fa-759b-4294-bf05-0b3f296a362b"
   },
   "source": [
    "- The model rightly identifies entities  including *but not limited to* `Hungary` and `Tuesday`.\n",
    "\n",
    "- However, it failed to recognise `Moscow` as an entity and misclassifies `said` and `energy` as entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf27f9-2769-4f7a-9bae-04a8789bde04",
   "metadata": {
    "id": "1fcf27f9-2769-4f7a-9bae-04a8789bde04"
   },
   "source": [
    "- I used `DistilBERT (uncased)` for **speed** and **simplicity**. In practice, a cased model like `bert-base-cased` often performs better on NER tasks, since capitalization matters (`‘Apple’` the company vs `‘apple’` the fruit).\n",
    "\n",
    "- But DistilBERT makes it easy to demo and deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87537d6-2260-46cb-b398-6d777feca15c",
   "metadata": {
    "id": "f87537d6-2260-46cb-b398-6d777feca15c"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "- Successfully fine-tuned DistilBERT for Named Entity Recognition.\n",
    "- Model identifies entities such as persons, locations, organizations, and dates.\n",
    "---\n",
    "**Usefulness of NER**:\n",
    "- Extracting structured data from unstructured text.\n",
    "- Enhancing search and recommendation systems.\n",
    "- Supporting legal, biomedical, and financial document analysis.\n",
    "- This project demonstrates how lightweight transformer models like DistilBERT can achieve strong performance on core NLP tasks.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
